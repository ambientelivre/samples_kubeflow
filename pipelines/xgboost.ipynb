{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b64584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "from kfp import components\n",
    "from kfp import dsl\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa4cab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hacks/kubeflow/lib/python3.9/site-packages/kfp/components/_components.py:196: FutureWarning: Container component must specify command to be compatible with KFP v2 compatible mode and emissary executor, which will be the default executor for KFP v2.https://www.kubeflow.org/docs/components/pipelines/installation/choose-executor/\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "diagnose_me_op = components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/566dddfdfc0a6a725b6e50ea85e73d8d5578bbb9/components/diagnostics/diagnose_me/component.yaml')\n",
    "\n",
    "confusion_matrix_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1.7.0/components/local/confusion_matrix/component.yaml')\n",
    "\n",
    "roc_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1.7.0/components/local/roc/component.yaml')\n",
    "\n",
    "dataproc_create_cluster_op = components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/1.7.0-rc.3/components/gcp/dataproc/create_cluster/component.yaml')\n",
    "\n",
    "dataproc_delete_cluster_op = components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/1.7.0-rc.3/components/gcp/dataproc/delete_cluster/component.yaml')\n",
    "\n",
    "dataproc_submit_pyspark_op = components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/1.7.0-rc.3/components/gcp/dataproc/submit_pyspark_job/component.yaml'\n",
    ")\n",
    "\n",
    "dataproc_submit_spark_op = components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/1.7.0-rc.3/components/gcp/dataproc/submit_spark_job/component.yaml'\n",
    ")\n",
    "\n",
    "_PYSRC_PREFIX = 'gs://ml-pipeline/sample-pipeline/xgboost' # Common path to python src.\n",
    "\n",
    "_XGBOOST_PKG = 'gs://ml-pipeline/sample-pipeline/xgboost/xgboost4j-example-0.8-SNAPSHOT-jar-with-dependencies.jar'\n",
    "\n",
    "_TRAINER_MAIN_CLS = 'ml.dmlc.xgboost4j.scala.example.spark.XGBoostTrainer'\n",
    "\n",
    "_PREDICTOR_MAIN_CLS = 'ml.dmlc.xgboost4j.scala.example.spark.XGBoostPredictor'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e9d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_directory_from_gcs(dir_path):\n",
    "  \"\"\"Delete a GCS dir recursively. Ignore errors.\"\"\"\n",
    "  try:\n",
    "    subprocess.call(['gsutil', '-m', 'rm', '-r', dir_path])\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# ! Please do not forget to enable the Dataproc API in your cluster https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview\n",
    "\n",
    "# ================================================================\n",
    "# The following classes should be provided by components provider.\n",
    "\n",
    "\n",
    "def dataproc_analyze_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    schema,\n",
    "    train_data,\n",
    "    output):\n",
    "  \"\"\"Submit dataproc analyze as a pyspark job.\n",
    "\n",
    "  :param project: GCP project ID.\n",
    "  :param region: Which zone to run this analyze.\n",
    "  :param cluster_name: Name of the cluster.\n",
    "  :param schema: GCS path to the schema.\n",
    "  :param train_data: GCS path to the training data.\n",
    "  :param output: GCS path to store the output.\n",
    "  \"\"\"\n",
    "  return dataproc_submit_pyspark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_python_file_uri=os.path.join(_PYSRC_PREFIX, 'analyze_run.py'),\n",
    "      args=['--output', str(output), '--train', str(train_data), '--schema', str(schema)]\n",
    "  )\n",
    "\n",
    "\n",
    "def dataproc_transform_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    train_data,\n",
    "    eval_data,\n",
    "    target,\n",
    "    analysis,\n",
    "    output\n",
    "):\n",
    "  \"\"\"Submit dataproc transform as a pyspark job.\n",
    "\n",
    "  :param project: GCP project ID.\n",
    "  :param region: Which zone to run this analyze.\n",
    "  :param cluster_name: Name of the cluster.\n",
    "  :param train_data: GCS path to the training data.\n",
    "  :param eval_data: GCS path of the eval csv file.\n",
    "  :param target: Target column name.\n",
    "  :param analysis: GCS path of the analysis results\n",
    "  :param output: GCS path to use for output.\n",
    "  \"\"\"\n",
    "\n",
    "  # Remove existing [output]/train and [output]/eval if they exist.\n",
    "  delete_directory_from_gcs(os.path.join(output, 'train'))\n",
    "  delete_directory_from_gcs(os.path.join(output, 'eval'))\n",
    "\n",
    "  return dataproc_submit_pyspark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_python_file_uri=os.path.join(_PYSRC_PREFIX,\n",
    "                                        'transform_run.py'),\n",
    "      args=[\n",
    "        '--output',\n",
    "        str(output),\n",
    "        '--analysis',\n",
    "        str(analysis),\n",
    "        '--target',\n",
    "        str(target),\n",
    "        '--train',\n",
    "        str(train_data),\n",
    "        '--eval',\n",
    "        str(eval_data)\n",
    "      ])\n",
    "\n",
    "\n",
    "def dataproc_train_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    train_data,\n",
    "    eval_data,\n",
    "    target,\n",
    "    analysis,\n",
    "    workers,\n",
    "    rounds,\n",
    "    output,\n",
    "    is_classification=True\n",
    "):\n",
    "\n",
    "  if is_classification:\n",
    "    config='gs://ml-pipeline/sample-data/xgboost-config/trainconfcla.json'\n",
    "  else:\n",
    "    config='gs://ml-pipeline/sample-data/xgboost-config/trainconfreg.json'\n",
    "\n",
    "  return dataproc_submit_spark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_class=_TRAINER_MAIN_CLS,\n",
    "      spark_job=json.dumps({'jarFileUris': [_XGBOOST_PKG]}),\n",
    "      args=json.dumps([\n",
    "        str(config),\n",
    "        str(rounds),\n",
    "        str(workers),\n",
    "        str(analysis),\n",
    "        str(target),\n",
    "        str(train_data),\n",
    "        str(eval_data),\n",
    "        str(output)\n",
    "      ]))\n",
    "\n",
    "\n",
    "def dataproc_predict_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    data,\n",
    "    model,\n",
    "    target,\n",
    "    analysis,\n",
    "    output\n",
    "):\n",
    "\n",
    "  return dataproc_submit_spark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_class=_PREDICTOR_MAIN_CLS,\n",
    "      spark_job=json.dumps({'jarFileUris': [_XGBOOST_PKG]}),\n",
    "      args=json.dumps([\n",
    "        str(model),\n",
    "        str(data),\n",
    "        str(analysis),\n",
    "        str(target),\n",
    "        str(output)\n",
    "      ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='xgboost-trainer',\n",
    "    description='A trainer that does end-to-end distributed training for XGBoost models.'\n",
    ")\n",
    "def xgb_train_pipeline(\n",
    "    output='gs://{{kfp-default-bucket}}',\n",
    "    project='{{kfp-project-id}}',\n",
    "    diagnostic_mode='HALT_ON_ERROR',\n",
    "    rounds=5,\n",
    "):\n",
    "    output_template = str(output) + '/' + dsl.RUN_ID_PLACEHOLDER + '/data'\n",
    "    region='us-central1'\n",
    "    workers=2\n",
    "    quota_check=[{'region':region,'metric':'CPUS','quota_needed':12.0}]\n",
    "    train_data='gs://ml-pipeline/sample-data/sfpd/train.csv'\n",
    "    eval_data='gs://ml-pipeline/sample-data/sfpd/eval.csv'\n",
    "    schema='gs://ml-pipeline/sample-data/sfpd/schema.json'\n",
    "    true_label='ACTION'\n",
    "    target='resolution'\n",
    "    required_apis='dataproc.googleapis.com'\n",
    "    cluster_name='xgb-%s' % dsl.RUN_ID_PLACEHOLDER\n",
    "\n",
    "    # Current GCP pyspark/spark op do not provide outputs as return values, instead,\n",
    "    # we need to use strings to pass the uri around.\n",
    "    analyze_output = output_template\n",
    "    transform_output_train = os.path.join(output_template, 'train', 'part-*')\n",
    "    transform_output_eval = os.path.join(output_template, 'eval', 'part-*')\n",
    "    train_output = os.path.join(output_template, 'train_output')\n",
    "    predict_output = os.path.join(output_template, 'predict_output')\n",
    "    \n",
    "    _diagnose_me_op = diagnose_me_op(\n",
    "        bucket=output,\n",
    "        execution_mode=diagnostic_mode,\n",
    "        project_id=project, \n",
    "        target_apis=required_apis,\n",
    "        quota_check=quota_check)\n",
    "    \n",
    "    with dsl.ExitHandler(exit_op=dataproc_delete_cluster_op(\n",
    "        project_id=project,\n",
    "        region=region,\n",
    "        name=cluster_name\n",
    "    )):\n",
    "        _create_cluster_op = dataproc_create_cluster_op(\n",
    "            project_id=project,\n",
    "            region=region,\n",
    "            name=cluster_name,\n",
    "            initialization_actions=[\n",
    "              os.path.join(_PYSRC_PREFIX,\n",
    "                           'initialization_actions.sh'),\n",
    "            ],\n",
    "            image_version='1.5'\n",
    "        ).after(_diagnose_me_op)\n",
    "\n",
    "        _analyze_op = dataproc_analyze_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            schema=schema,\n",
    "            train_data=train_data,\n",
    "            output=output_template\n",
    "        ).after(_create_cluster_op).set_display_name('Analyzer')\n",
    "\n",
    "        _transform_op = dataproc_transform_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            train_data=train_data,\n",
    "            eval_data=eval_data,\n",
    "            target=target,\n",
    "            analysis=analyze_output,\n",
    "            output=output_template\n",
    "        ).after(_analyze_op).set_display_name('Transformer')\n",
    "\n",
    "        _train_op = dataproc_train_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            train_data=transform_output_train,\n",
    "            eval_data=transform_output_eval,\n",
    "            target=target,\n",
    "            analysis=analyze_output,\n",
    "            workers=workers,\n",
    "            rounds=rounds,\n",
    "            output=train_output\n",
    "        ).after(_transform_op).set_display_name('Trainer')\n",
    "\n",
    "        _predict_op = dataproc_predict_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            data=transform_output_eval,\n",
    "            model=train_output,\n",
    "            target=target,\n",
    "            analysis=analyze_output,\n",
    "            output=predict_output\n",
    "        ).after(_train_op).set_display_name('Predictor')\n",
    "\n",
    "        _cm_op = confusion_matrix_op(\n",
    "            predictions=os.path.join(predict_output, 'part-*.csv'),\n",
    "            output_dir=output_template\n",
    "        ).after(_predict_op)\n",
    "\n",
    "        _roc_op = roc_op(\n",
    "            predictions_dir=os.path.join(predict_output, 'part-*.csv'),\n",
    "            true_class=true_label,\n",
    "            true_score_column=true_label,\n",
    "            output_dir=output_template\n",
    "        ).after(_predict_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b8dea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hacks/kubeflow/lib/python3.9/site-packages/kfp/components/_data_passing.py:227: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"5\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kfp.compiler.Compiler().compile(xgb_train_pipeline, 'xgboost' + '.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8831c727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
