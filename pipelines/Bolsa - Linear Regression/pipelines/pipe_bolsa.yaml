apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: pipeline-previsao-dolar-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2023-03-31T09:54:59.545161',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Dados Bolsa X Dolar",
      "inputs": [{"name": "feat"}, {"name": "label"}, {"name": "valor"}, {"name":
      "dataset"}], "name": "Pipeline Previsao Dolar"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: pipeline-previsao-dolar
  templates:
  - name: pipeline-previsao-dolar
    inputs:
      parameters:
      - {name: dataset}
      - {name: feat}
      - {name: label}
      - {name: valor}
    dag:
      tasks:
      - name: previsao
        template: previsao
        dependencies: [treina]
        arguments:
          parameters:
          - {name: feat, value: '{{inputs.parameters.feat}}'}
          - {name: label, value: '{{inputs.parameters.label}}'}
          - {name: valor, value: '{{inputs.parameters.valor}}'}
          artifacts:
          - {name: treina-output_model, from: '{{tasks.treina.outputs.artifacts.treina-output_model}}'}
      - name: treina
        template: treina
        arguments:
          parameters:
          - {name: dataset, value: '{{inputs.parameters.dataset}}'}
          - {name: feat, value: '{{inputs.parameters.feat}}'}
          - {name: label, value: '{{inputs.parameters.label}}'}
      - name: valida
        template: valida
        dependencies: [treina]
        arguments:
          parameters:
          - {name: dataset, value: '{{inputs.parameters.dataset}}'}
          - {name: feat, value: '{{inputs.parameters.feat}}'}
          - {name: label, value: '{{inputs.parameters.label}}'}
          artifacts:
          - {name: treina-output_model, from: '{{tasks.treina.outputs.artifacts.treina-output_model}}'}
  - name: previsao
    container:
      args: [--feat, '{{inputs.parameters.feat}}', --label, '{{inputs.parameters.label}}',
        --model, /tmp/inputs/model/data, --valor, '{{inputs.parameters.valor}}', '----output-paths',
        /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def previsao(feat, label, model_path, valor):\n\n    import pandas as pd\n\
        \    import pickle\n\n    future = pd.DataFrame({feat:[valor]})\n    with\
        \ open(model_path, 'rb') as f:\n        trained_model = pickle.load(f)\n\n\
        \    # Retorna a previs\xE3o do valor futuro do d\xF3lar quando a bolsa atinge\
        \ ${valor} pontos\n\n    return trained_model.predict(future)[0]\n\ndef _serialize_float(float_value:\
        \ float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n\
        \    if not isinstance(float_value, (float, int)):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of float.'.format(\n            str(float_value),\
        \ str(type(float_value))))\n    return str(float_value)\n\nimport argparse\n\
        _parser = argparse.ArgumentParser(prog='Previsao', description='')\n_parser.add_argument(\"\
        --feat\", dest=\"feat\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--label\", dest=\"label\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\", dest=\"model_path\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --valor\", dest=\"valor\", type=float, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = previsao(**_parsed_args)\n\n_outputs =\
        \ [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport\
        \ os\nfor idx, output_file in enumerate(_output_files):\n    try:\n      \
        \  os.makedirs(os.path.dirname(output_file))\n    except OSError:\n      \
        \  pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: tensorflow/tensorflow:1.11.0-py3
    inputs:
      parameters:
      - {name: feat}
      - {name: label}
      - {name: valor}
      artifacts:
      - {name: treina-output_model, path: /tmp/inputs/model/data}
    outputs:
      artifacts:
      - {name: previsao-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--feat", {"inputValue": "feat"}, "--label", {"inputValue": "label"},
          "--model", {"inputPath": "model"}, "--valor", {"inputValue": "valor"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def previsao(feat, label, model_path, valor):\n\n    import pandas as pd\n    import
          pickle\n\n    future = pd.DataFrame({feat:[valor]})\n    with open(model_path,
          ''rb'') as f:\n        trained_model = pickle.load(f)\n\n    # Retorna a
          previs\u00e3o do valor futuro do d\u00f3lar quando a bolsa atinge ${valor}
          pontos\n\n    return trained_model.predict(future)[0]\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(\n            str(float_value),
          str(type(float_value))))\n    return str(float_value)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Previsao'', description='''')\n_parser.add_argument(\"--feat\",
          dest=\"feat\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--label\",
          dest=\"label\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--valor\",
          dest=\"valor\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = previsao(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "tensorflow/tensorflow:1.11.0-py3"}}, "inputs": [{"name": "feat",
          "type": "String"}, {"name": "label", "type": "String"}, {"name": "model"},
          {"name": "valor", "type": "Float"}], "name": "Previsao", "outputs": [{"name":
          "Output", "type": "Float"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"feat": "{{inputs.parameters.feat}}",
          "label": "{{inputs.parameters.label}}", "valor": "{{inputs.parameters.valor}}"}'}
  - name: treina
    container:
      args: [--feat, '{{inputs.parameters.feat}}', --label, '{{inputs.parameters.label}}',
        --dataset, '{{inputs.parameters.dataset}}', --output-model, /tmp/outputs/output_model/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef treina(feat, label, dataset, output_model_path) :\n    \"\"\"\n    A\
        \ fun\xE7\xE3o retorna o caminho at\xE9 o bin\xE1rio do arquivo que cont\xE9\
        m o modelo treinado.\n    Isso fica impl\xEDcito quando declaramos OutputPath(str).\n\
        \    \"\"\"\n\n    import pickle\n    import pandas as pd\n    from sklearn.linear_model\
        \ import LinearRegression\n\n    # Recebe o dataset e treina o algoritmo\n\
        \    df = pd.read_csv(dataset)\n    reglin = LinearRegression()\n    reglin.fit(df[[feat]],\
        \ df[label])\n\n    # Registra o bin\xE1rio do modelo treinado no caminho\
        \ at\xE9 o arquivo \"output_model_path\" no minIO\n    with open(output_model_path,\
        \ 'wb') as f:\n        pickle.dump(reglin, f)\n\nimport argparse\n_parser\
        \ = argparse.ArgumentParser(prog='Treina', description='A fun\xE7\xE3o retorna\
        \ o caminho at\xE9 o bin\xE1rio do arquivo que cont\xE9m o modelo treinado.')\n\
        _parser.add_argument(\"--feat\", dest=\"feat\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--label\", dest=\"label\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataset\", dest=\"\
        dataset\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --output-model\", dest=\"output_model_path\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = treina(**_parsed_args)\n"
      image: tensorflow/tensorflow:1.11.0-py3
    inputs:
      parameters:
      - {name: dataset}
      - {name: feat}
      - {name: label}
    outputs:
      artifacts:
      - {name: treina-output_model, path: /tmp/outputs/output_model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "A fun\u00e7\u00e3o
          retorna o caminho at\u00e9 o bin\u00e1rio do arquivo que cont\u00e9m o modelo
          treinado.", "implementation": {"container": {"args": ["--feat", {"inputValue":
          "feat"}, "--label", {"inputValue": "label"}, "--dataset", {"inputValue":
          "dataset"}, "--output-model", {"outputPath": "output_model"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef treina(feat, label, dataset, output_model_path) :\n    \"\"\"\n    A
          fun\u00e7\u00e3o retorna o caminho at\u00e9 o bin\u00e1rio do arquivo que
          cont\u00e9m o modelo treinado.\n    Isso fica impl\u00edcito quando declaramos
          OutputPath(str).\n    \"\"\"\n\n    import pickle\n    import pandas as
          pd\n    from sklearn.linear_model import LinearRegression\n\n    # Recebe
          o dataset e treina o algoritmo\n    df = pd.read_csv(dataset)\n    reglin
          = LinearRegression()\n    reglin.fit(df[[feat]], df[label])\n\n    # Registra
          o bin\u00e1rio do modelo treinado no caminho at\u00e9 o arquivo \"output_model_path\"
          no minIO\n    with open(output_model_path, ''wb'') as f:\n        pickle.dump(reglin,
          f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Treina'',
          description=''A fun\u00e7\u00e3o retorna o caminho at\u00e9 o bin\u00e1rio
          do arquivo que cont\u00e9m o modelo treinado.'')\n_parser.add_argument(\"--feat\",
          dest=\"feat\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--label\",
          dest=\"label\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataset\",
          dest=\"dataset\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-model\",
          dest=\"output_model_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = treina(**_parsed_args)\n"], "image": "tensorflow/tensorflow:1.11.0-py3"}},
          "inputs": [{"name": "feat", "type": "String"}, {"name": "label", "type":
          "String"}, {"name": "dataset", "type": "String"}], "name": "Treina", "outputs":
          [{"name": "output_model", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dataset": "{{inputs.parameters.dataset}}",
          "feat": "{{inputs.parameters.feat}}", "label": "{{inputs.parameters.label}}"}'}
  - name: valida
    container:
      args: [--feat, '{{inputs.parameters.feat}}', --label, '{{inputs.parameters.label}}',
        --dataset, '{{inputs.parameters.dataset}}', --model, /tmp/inputs/model/data,
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def valida(feat, label, dataset, model_path):\n\n    import pickle\n    import\
        \ pandas as pd\n    from sklearn.metrics import mean_squared_error, r2_score,\
        \ mean_absolute_error\n\n    # Importando o dataset\n    df = pd.read_csv(dataset)\n\
        \    y_true = df[label]\n\n    # Importando o modelo serializado como um objeto\
        \ python\n    with open(model_path, 'rb') as f:\n        trained_model = pickle.load(f)\n\
        \n    y_pred = trained_model.predict(df[[feat]])\n\n    # Calcula as m\xE9\
        tricas de desempenho do modelo para o problema de regress\xE3o\n    r2 = r2_score(y_true,\
        \ y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    mae = mean_absolute_error(y_true,\
        \ y_pred)\n\n    valida_metrics = {\n        'r2' : r2,\n        'mse' : mse,\n\
        \        'mae' : mae\n    }\n\n    # Retorna um dicion\xE1rio com o c\xE1\
        lculo das m\xE9tricas\n\n    return valida_metrics\n\ndef _serialize_json(obj)\
        \ -> str:\n    if isinstance(obj, str):\n        return obj\n    import json\n\
        \n    def default_serializer(obj):\n        if hasattr(obj, 'to_struct'):\n\
        \            return obj.to_struct()\n        else:\n            raise TypeError(\n\
        \                \"Object of type '%s' is not JSON serializable and does not\
        \ have .to_struct() method.\"\n                % obj.__class__.__name__)\n\
        \n    return json.dumps(obj, default=default_serializer, sort_keys=True)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Valida', description='')\n\
        _parser.add_argument(\"--feat\", dest=\"feat\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--label\", dest=\"label\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataset\", dest=\"\
        dataset\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = valida(**_parsed_args)\n\n_outputs = [_outputs]\n\
        \n_output_serializers = [\n    _serialize_json,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: tensorflow/tensorflow:1.11.0-py3
    inputs:
      parameters:
      - {name: dataset}
      - {name: feat}
      - {name: label}
      artifacts:
      - {name: treina-output_model, path: /tmp/inputs/model/data}
    outputs:
      artifacts:
      - {name: valida-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--feat", {"inputValue": "feat"}, "--label", {"inputValue": "label"},
          "--dataset", {"inputValue": "dataset"}, "--model", {"inputPath": "model"},
          "----output-paths", {"outputPath": "Output"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def valida(feat, label, dataset, model_path):\n\n    import
          pickle\n    import pandas as pd\n    from sklearn.metrics import mean_squared_error,
          r2_score, mean_absolute_error\n\n    # Importando o dataset\n    df = pd.read_csv(dataset)\n    y_true
          = df[label]\n\n    # Importando o modelo serializado como um objeto python\n    with
          open(model_path, ''rb'') as f:\n        trained_model = pickle.load(f)\n\n    y_pred
          = trained_model.predict(df[[feat]])\n\n    # Calcula as m\u00e9tricas de
          desempenho do modelo para o problema de regress\u00e3o\n    r2 = r2_score(y_true,
          y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    mae = mean_absolute_error(y_true,
          y_pred)\n\n    valida_metrics = {\n        ''r2'' : r2,\n        ''mse''
          : mse,\n        ''mae'' : mae\n    }\n\n    # Retorna um dicion\u00e1rio
          com o c\u00e1lculo das m\u00e9tricas\n\n    return valida_metrics\n\ndef
          _serialize_json(obj) -> str:\n    if isinstance(obj, str):\n        return
          obj\n    import json\n\n    def default_serializer(obj):\n        if hasattr(obj,
          ''to_struct''):\n            return obj.to_struct()\n        else:\n            raise
          TypeError(\n                \"Object of type ''%s'' is not JSON serializable
          and does not have .to_struct() method.\"\n                % obj.__class__.__name__)\n\n    return
          json.dumps(obj, default=default_serializer, sort_keys=True)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Valida'', description='''')\n_parser.add_argument(\"--feat\",
          dest=\"feat\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--label\",
          dest=\"label\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataset\",
          dest=\"dataset\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = valida(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_json,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "tensorflow/tensorflow:1.11.0-py3"}}, "inputs": [{"name": "feat",
          "type": "String"}, {"name": "label", "type": "String"}, {"name": "dataset",
          "type": "String"}, {"name": "model"}], "name": "Valida", "outputs": [{"name":
          "Output", "type": "typing.Dict[str, float]"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dataset": "{{inputs.parameters.dataset}}",
          "feat": "{{inputs.parameters.feat}}", "label": "{{inputs.parameters.label}}"}'}
  arguments:
    parameters:
    - {name: feat}
    - {name: label}
    - {name: valor}
    - {name: dataset}
  serviceAccountName: pipeline-runner
